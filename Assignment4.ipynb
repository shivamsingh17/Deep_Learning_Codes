{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/shivamsingh17/Deep_Learning_Codes/blob/master/Assignment4.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "CCetzbBTRWC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        },
        "outputId": "6366e26b-cd27-4e9c-bffa-8a0972cecc29"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "def f(w, b, x):\n",
        "    return 1.0 / (1.0 + np.exp(-(w*x + b)))\n",
        "def error(w, b): #Calculate the error\n",
        "    err = 0.0\n",
        "    for x,y in zip(X,Y) :\n",
        "        fx = f(w, b, x)\n",
        "        err += 0.5 * (fx - y) ** 2\n",
        "    return err\n",
        "def grad_b(w, b, x, y):\n",
        "    fx = f(w, b, x)\n",
        "    return (fx - y) * fx * (1 - fx)\n",
        "def grad_w(w, b, x, y):\n",
        "    fx = f(w, b, x)\n",
        "    return (fx - y) * fx * (1 - fx) * x\n",
        "  \n",
        "def do_gradient_descent(X, Y, w, b, eta, max_epochs): #Gradient Descent\n",
        "    dw = 0\n",
        "    db = 0\n",
        "    for i in range(max_epochs):\n",
        "        for x,y in zip(X,Y):\n",
        "            dw += grad_w(w, b, x, y)\n",
        "            db += grad_b(w, b, x, y)\n",
        "        w = w - eta * dw\n",
        "        b = b - eta * db\n",
        "        print(\"Epoch {} : Loss = {}\".format(i, error(w, b)))\n",
        "    return w,b\n",
        "  \n",
        "\n",
        "def do_adam(X, Y, init_w, init_b, eta, max_epochs):\n",
        "  \n",
        "  w_history, b_history, error_history = [], [], []\n",
        "  w, b, eta,mini_batch_size, num_points_seen = init_w, init_b, eta, 10, 0\n",
        "  m_w, m_b, v_w, v_b,m_w_hat, m_b_hat, v_w_hat, v_b_hat, eps, beta1, beta2 = 0, 0, 0, 0,0,0,0,0, 1e-8, 0.9, 0.999\n",
        "  for i in range(max_epochs):\n",
        "    dw, db = 0, 0\n",
        "    for x,y in zip(X,Y):\n",
        "      dw += grad_w(w, b, x, y)\n",
        "      db += grad_b(w, b, x, y)\n",
        "      \n",
        "    m_w = beta1 * m_w + (1-beta1) * dw\n",
        "    m_b = beta1 * m_b + (1-beta1) * db\n",
        "    \n",
        "    v_w = beta2 * v_w + (1-beta2) * dw ** 2\n",
        "    v_b = beta2 * v_b + (1-beta2) * db ** 2\n",
        "    #Biias correction\n",
        "    m_w_hat = m_w / (1 - math.pow(beta1, i+1))\n",
        "    m_b_hat = m_b / (1 - math.pow(beta1, i+1))\n",
        "                              \n",
        "    v_w_hat = v_w / (1 - math.pow(beta2, i+1))\n",
        "    v_b_hat = v_b / (1 - math.pow(beta2, i+1))\n",
        "    \n",
        "    w = w - (eta / np.sqrt(v_w_hat + eps)) * m_w_hat\n",
        "    b = b - (eta / np.sqrt(v_b_hat + eps)) * m_b_hat\n",
        "    print(\"Epoch {} : Loss_adam = {}\".format(i, error(w, b)))\n",
        "  return w, b                          \n",
        "                              \n",
        "if __name__ == \"__main__\":\n",
        "    filename = 'A4_Q7_data.csv'\n",
        "    df = pd.read_csv(filename)\n",
        "    X = df['X']\n",
        "    Y = df['Y']\n",
        "    initial_w = 1\n",
        "    initial_b = 1\n",
        "    eta = 0.01\n",
        "    max_epochs = 100\n",
        "    w, b = do_adam(X, Y, initial_w, initial_b, eta, max_epochs)\n",
        "    error = error(w,b)\n",
        "    print(\"error ={}\".format(error))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 : Loss_adam = 0.05937623071987896\n",
            "Epoch 1 : Loss_adam = 0.056477810013398196\n",
            "Epoch 2 : Loss_adam = 0.05370424881930501\n",
            "Epoch 3 : Loss_adam = 0.05105339542800927\n",
            "Epoch 4 : Loss_adam = 0.04852289436128088\n",
            "Epoch 5 : Loss_adam = 0.04611019999282507\n",
            "Epoch 6 : Loss_adam = 0.04381259118801056\n",
            "Epoch 7 : Loss_adam = 0.041627186783459134\n",
            "Epoch 8 : Loss_adam = 0.03955096172338526\n",
            "Epoch 9 : Loss_adam = 0.037580763669344\n",
            "Epoch 10 : Loss_adam = 0.03571332990314388\n",
            "Epoch 11 : Loss_adam = 0.033945304348768024\n",
            "Epoch 12 : Loss_adam = 0.032273254547818486\n",
            "Epoch 13 : Loss_adam = 0.03069368843381597\n",
            "Epoch 14 : Loss_adam = 0.029203070763188933\n",
            "Epoch 15 : Loss_adam = 0.027797839074522695\n",
            "Epoch 16 : Loss_adam = 0.026474419062174178\n",
            "Epoch 17 : Loss_adam = 0.02522923926529587\n",
            "Epoch 18 : Loss_adam = 0.024058744988298558\n",
            "Epoch 19 : Loss_adam = 0.02295941138351677\n",
            "Epoch 20 : Loss_adam = 0.021927755641075446\n",
            "Epoch 21 : Loss_adam = 0.020960348244498682\n",
            "Epoch 22 : Loss_adam = 0.020053823263309577\n",
            "Epoch 23 : Loss_adam = 0.019204887665647638\n",
            "Epoch 24 : Loss_adam = 0.018410329644723326\n",
            "Epoch 25 : Loss_adam = 0.0176670259627131\n",
            "Epoch 26 : Loss_adam = 0.016971948324479985\n",
            "Epoch 27 : Loss_adam = 0.01632216880130463\n",
            "Epoch 28 : Loss_adam = 0.015714864331668476\n",
            "Epoch 29 : Loss_adam = 0.015147320332089105\n",
            "Epoch 30 : Loss_adam = 0.014616933456115125\n",
            "Epoch 31 : Loss_adam = 0.014121213543895476\n",
            "Epoch 32 : Loss_adam = 0.013657784808292602\n",
            "Epoch 33 : Loss_adam = 0.013224386306355466\n",
            "Epoch 34 : Loss_adam = 0.012818871747149926\n",
            "Epoch 35 : Loss_adam = 0.012439208688499341\n",
            "Epoch 36 : Loss_adam = 0.01208347717615353\n",
            "Epoch 37 : Loss_adam = 0.011749867879316766\n",
            "Epoch 38 : Loss_adam = 0.01143667977635652\n",
            "Epoch 39 : Loss_adam = 0.01114231744392535\n",
            "Epoch 40 : Loss_adam = 0.010865288001688548\n",
            "Epoch 41 : Loss_adam = 0.010604197763403942\n",
            "Epoch 42 : Loss_adam = 0.010357748643286046\n",
            "Epoch 43 : Loss_adam = 0.01012473436444532\n",
            "Epoch 44 : Loss_adam = 0.00990403651377399\n",
            "Epoch 45 : Loss_adam = 0.009694620484993925\n",
            "Epoch 46 : Loss_adam = 0.00949553134873911\n",
            "Epoch 47 : Loss_adam = 0.009305889685562292\n",
            "Epoch 48 : Loss_adam = 0.009124887414676172\n",
            "Epoch 49 : Loss_adam = 0.008951783648112343\n",
            "Epoch 50 : Loss_adam = 0.008785900596843907\n",
            "Epoch 51 : Loss_adam = 0.008626619552313475\n",
            "Epoch 52 : Loss_adam = 0.008473376963769503\n",
            "Epoch 53 : Loss_adam = 0.008325660628873148\n",
            "Epoch 54 : Loss_adam = 0.008183006012222998\n",
            "Epoch 55 : Loss_adam = 0.008044992703775435\n",
            "Epoch 56 : Loss_adam = 0.00791124102663617\n",
            "Epoch 57 : Loss_adam = 0.007781408801372564\n",
            "Epoch 58 : Loss_adam = 0.007655188271859731\n",
            "Epoch 59 : Loss_adam = 0.007532303195728153\n",
            "Epoch 60 : Loss_adam = 0.007412506100730622\n",
            "Epoch 61 : Loss_adam = 0.007295575706788637\n",
            "Epoch 62 : Loss_adam = 0.007181314512109495\n",
            "Epoch 63 : Loss_adam = 0.007069546540578068\n",
            "Epoch 64 : Loss_adam = 0.006960115246614306\n",
            "Epoch 65 : Loss_adam = 0.0068528815728364275\n",
            "Epoch 66 : Loss_adam = 0.006747722155173992\n",
            "Epoch 67 : Loss_adam = 0.006644527669517164\n",
            "Epoch 68 : Loss_adam = 0.006543201313562208\n",
            "Epoch 69 : Loss_adam = 0.006443657417201873\n",
            "Epoch 70 : Loss_adam = 0.006345820174603495\n",
            "Epoch 71 : Loss_adam = 0.006249622491003802\n",
            "Epoch 72 : Loss_adam = 0.006155004937218181\n",
            "Epoch 73 : Loss_adam = 0.0060619148048998706\n",
            "Epoch 74 : Loss_adam = 0.005970305255684235\n",
            "Epoch 75 : Loss_adam = 0.005880134557502736\n",
            "Epoch 76 : Loss_adam = 0.005791365401543727\n",
            "Epoch 77 : Loss_adam = 0.0057039642935634936\n",
            "Epoch 78 : Loss_adam = 0.005617901013504288\n",
            "Epoch 79 : Loss_adam = 0.005533148137650808\n",
            "Epoch 80 : Loss_adam = 0.005449680617844531\n",
            "Epoch 81 : Loss_adam = 0.005367475412575125\n",
            "Epoch 82 : Loss_adam = 0.0052865111650711874\n",
            "Epoch 83 : Loss_adam = 0.005206767923818934\n",
            "Epoch 84 : Loss_adam = 0.005128226901240543\n",
            "Epoch 85 : Loss_adam = 0.00505087026656384\n",
            "Epoch 86 : Loss_adam = 0.004974680969206615\n",
            "Epoch 87 : Loss_adam = 0.004899642589283311\n",
            "Epoch 88 : Loss_adam = 0.004825739212114753\n",
            "Epoch 89 : Loss_adam = 0.0047529553238838135\n",
            "Epoch 90 : Loss_adam = 0.0046812757258297534\n",
            "Epoch 91 : Loss_adam = 0.004610685464610494\n",
            "Epoch 92 : Loss_adam = 0.004541169776686059\n",
            "Epoch 93 : Loss_adam = 0.004472714044785827\n",
            "Epoch 94 : Loss_adam = 0.00440530376471924\n",
            "Epoch 95 : Loss_adam = 0.004338924520972187\n",
            "Epoch 96 : Loss_adam = 0.004273561969701266\n",
            "Epoch 97 : Loss_adam = 0.0042092018278949185\n",
            "Epoch 98 : Loss_adam = 0.004145829867614911\n",
            "Epoch 99 : Loss_adam = 0.00408343191436398\n",
            "error =0.00408343191436398\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}