{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adam.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/shivamsingh17/Deep_Learning_Codes/blob/master/assignment_4.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "CCetzbBTRWC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        },
        "outputId": "094cd2d6-e62d-4193-c65c-022c1038f6a0"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "def f(w, b, x):\n",
        "    return 1.0 / (1.0 + np.exp(-(w*x + b)))\n",
        "def error(w, b): #Calculate the error\n",
        "    err = 0.0\n",
        "    for x,y in zip(X,Y) :\n",
        "        fx = f(w, b, x)\n",
        "        err += 0.5 * (fx - y) ** 2\n",
        "    return err\n",
        "def grad_b(w, b, x, y):\n",
        "    fx = f(w, b, x)\n",
        "    return (fx - y) * fx * (1 - fx)\n",
        "def grad_w(w, b, x, y):\n",
        "    fx = f(w, b, x)\n",
        "    return (fx - y) * fx * (1 - fx) * x\n",
        "  \n",
        "def do_gradient_descent(X, Y, w, b, eta, max_epochs): #Gradient Descent\n",
        "    dw = 0\n",
        "    db = 0\n",
        "    for i in range(max_epochs):\n",
        "        for x,y in zip(X,Y):\n",
        "            dw += grad_w(w, b, x, y)\n",
        "            db += grad_b(w, b, x, y)\n",
        "        w = w - eta * dw\n",
        "        b = b - eta * db\n",
        "        print(\"Epoch {} : Loss = {}\".format(i, error(w, b)))\n",
        "    return w,b\n",
        "def do_adam(X, Y, init_w, init_b, eta, max_epochs):\n",
        "  \n",
        "  w_history, b_history, error_history = [], [], []\n",
        "  w, b, eta,mini_batch_size, num_points_seen = init_w, init_b, 0.01, 10, 0\n",
        "  m_w, m_b, v_w, v_b, eps, beta1, beta2 = 0, 0, 0, 0, 0, 0.9, 0.99\n",
        "  for i in range(max_epochs):\n",
        "    dw, db = 0, 0\n",
        "    for x,y in zip(X,Y):\n",
        "      dw += grad_w(w, b, x, y)\n",
        "      db += grad_b(w, b, x, y)\n",
        "      \n",
        "    m_w = beta1 * m_w + (1-beta1) * dw\n",
        "    m_b = beta1 * m_b + (1-beta1) * db\n",
        "    \n",
        "    v_w = beta2 * v_w + (1-beta2) * dw ** 2\n",
        "    v_b = beta2 * v_b + (1-beta2) * db ** 2\n",
        "    \n",
        "    m_w = m_w / (1 - math.pow(beta1, i+1))\n",
        "    m_b = m_b / (1 - math.pow(beta1, i+1))\n",
        "                              \n",
        "    v_w = v_w / (1 - math.pow(beta2, i+1))\n",
        "    v_b = v_b / (1 - math.pow(beta2, i+1))\n",
        "    \n",
        "    w = w - (eta / np.sqrt(v_w + eps)) * m_w\n",
        "    b = b - (eta / np.sqrt(v_b + eps)) * m_b\n",
        "    print(\"Epoch {} : Loss_adam = {}\".format(i, error(w, b)))\n",
        "  return w, b                          \n",
        "                              \n",
        "if __name__ == \"__main__\":\n",
        "    filename = 'A4_Q7_data.csv'\n",
        "    df = pd.read_csv(filename)\n",
        "    X = df['X']\n",
        "    Y = df['Y']\n",
        "    initial_w = 1\n",
        "    initial_b = 1\n",
        "    eta = 0.01\n",
        "    max_epochs = 100\n",
        "    w, b = do_adam(X, Y, initial_w, initial_b, eta, max_epochs)\n",
        "    error = error(w,b)\n",
        "    print(\"error ={}\".format(error))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 : Loss_adam = 0.05937623003051502\n",
            "Epoch 1 : Loss_adam = 0.05721778651026982\n",
            "Epoch 2 : Loss_adam = 0.05598252700066504\n",
            "Epoch 3 : Loss_adam = 0.05534285752504303\n",
            "Epoch 4 : Loss_adam = 0.05503162475131681\n",
            "Epoch 5 : Loss_adam = 0.054886641191315734\n",
            "Epoch 6 : Loss_adam = 0.05482119409384296\n",
            "Epoch 7 : Loss_adam = 0.05479231552313216\n",
            "Epoch 8 : Loss_adam = 0.05477977700916931\n",
            "Epoch 9 : Loss_adam = 0.054774392570435086\n",
            "Epoch 10 : Loss_adam = 0.05477209626636316\n",
            "Epoch 11 : Loss_adam = 0.054771120542092745\n",
            "Epoch 12 : Loss_adam = 0.05477070637842131\n",
            "Epoch 13 : Loss_adam = 0.0547705303890875\n",
            "Epoch 14 : Loss_adam = 0.05477045539605565\n",
            "Epoch 15 : Loss_adam = 0.05477042330464316\n",
            "Epoch 16 : Loss_adam = 0.054770409497887675\n",
            "Epoch 17 : Loss_adam = 0.054770403520114246\n",
            "Epoch 18 : Loss_adam = 0.054770400913558746\n",
            "Epoch 19 : Loss_adam = 0.0547703997681879\n",
            "Epoch 20 : Loss_adam = 0.05477039926073421\n",
            "Epoch 21 : Loss_adam = 0.05477039903396039\n",
            "Epoch 22 : Loss_adam = 0.05477039893170799\n",
            "Epoch 23 : Loss_adam = 0.05477039888517668\n",
            "Epoch 24 : Loss_adam = 0.05477039886380268\n",
            "Epoch 25 : Loss_adam = 0.054770398853890845\n",
            "Epoch 26 : Loss_adam = 0.054770398849250106\n",
            "Epoch 27 : Loss_adam = 0.054770398847056354\n",
            "Epoch 28 : Loss_adam = 0.054770398846009184\n",
            "Epoch 29 : Loss_adam = 0.05477039884550451\n",
            "Epoch 30 : Loss_adam = 0.054770398845259\n",
            "Epoch 31 : Loss_adam = 0.05477039884513837\n",
            "Epoch 32 : Loss_adam = 0.05477039884507853\n",
            "Epoch 33 : Loss_adam = 0.05477039884504857\n",
            "Epoch 34 : Loss_adam = 0.05477039884503346\n",
            "Epoch 35 : Loss_adam = 0.05477039884502575\n",
            "Epoch 36 : Loss_adam = 0.05477039884502179\n",
            "Epoch 37 : Loss_adam = 0.05477039884501966\n",
            "Epoch 38 : Loss_adam = 0.05477039884501859\n",
            "Epoch 39 : Loss_adam = 0.05477039884501803\n",
            "Epoch 40 : Loss_adam = 0.05477039884501771\n",
            "Epoch 41 : Loss_adam = 0.05477039884501756\n",
            "Epoch 42 : Loss_adam = 0.05477039884501747\n",
            "Epoch 43 : Loss_adam = 0.05477039884501743\n",
            "Epoch 44 : Loss_adam = 0.05477039884501741\n",
            "Epoch 45 : Loss_adam = 0.05477039884501741\n",
            "Epoch 46 : Loss_adam = 0.05477039884501741\n",
            "Epoch 47 : Loss_adam = 0.05477039884501741\n",
            "Epoch 48 : Loss_adam = 0.05477039884501741\n",
            "Epoch 49 : Loss_adam = 0.05477039884501741\n",
            "Epoch 50 : Loss_adam = 0.05477039884501741\n",
            "Epoch 51 : Loss_adam = 0.05477039884501741\n",
            "Epoch 52 : Loss_adam = 0.05477039884501741\n",
            "Epoch 53 : Loss_adam = 0.05477039884501741\n",
            "Epoch 54 : Loss_adam = 0.05477039884501741\n",
            "Epoch 55 : Loss_adam = 0.05477039884501741\n",
            "Epoch 56 : Loss_adam = 0.05477039884501741\n",
            "Epoch 57 : Loss_adam = 0.05477039884501741\n",
            "Epoch 58 : Loss_adam = 0.05477039884501741\n",
            "Epoch 59 : Loss_adam = 0.05477039884501741\n",
            "Epoch 60 : Loss_adam = 0.05477039884501741\n",
            "Epoch 61 : Loss_adam = 0.05477039884501741\n",
            "Epoch 62 : Loss_adam = 0.05477039884501741\n",
            "Epoch 63 : Loss_adam = 0.05477039884501741\n",
            "Epoch 64 : Loss_adam = 0.05477039884501741\n",
            "Epoch 65 : Loss_adam = 0.05477039884501741\n",
            "Epoch 66 : Loss_adam = 0.05477039884501741\n",
            "Epoch 67 : Loss_adam = 0.05477039884501741\n",
            "Epoch 68 : Loss_adam = 0.05477039884501741\n",
            "Epoch 69 : Loss_adam = 0.05477039884501741\n",
            "Epoch 70 : Loss_adam = 0.05477039884501741\n",
            "Epoch 71 : Loss_adam = 0.05477039884501741\n",
            "Epoch 72 : Loss_adam = 0.05477039884501741\n",
            "Epoch 73 : Loss_adam = 0.05477039884501741\n",
            "Epoch 74 : Loss_adam = 0.05477039884501741\n",
            "Epoch 75 : Loss_adam = 0.05477039884501741\n",
            "Epoch 76 : Loss_adam = 0.05477039884501741\n",
            "Epoch 77 : Loss_adam = 0.05477039884501741\n",
            "Epoch 78 : Loss_adam = 0.05477039884501741\n",
            "Epoch 79 : Loss_adam = 0.05477039884501741\n",
            "Epoch 80 : Loss_adam = 0.05477039884501741\n",
            "Epoch 81 : Loss_adam = 0.05477039884501741\n",
            "Epoch 82 : Loss_adam = 0.05477039884501741\n",
            "Epoch 83 : Loss_adam = 0.05477039884501741\n",
            "Epoch 84 : Loss_adam = 0.05477039884501741\n",
            "Epoch 85 : Loss_adam = 0.05477039884501741\n",
            "Epoch 86 : Loss_adam = 0.05477039884501741\n",
            "Epoch 87 : Loss_adam = 0.05477039884501741\n",
            "Epoch 88 : Loss_adam = 0.05477039884501741\n",
            "Epoch 89 : Loss_adam = 0.05477039884501741\n",
            "Epoch 90 : Loss_adam = 0.05477039884501741\n",
            "Epoch 91 : Loss_adam = 0.05477039884501741\n",
            "Epoch 92 : Loss_adam = 0.05477039884501741\n",
            "Epoch 93 : Loss_adam = 0.05477039884501741\n",
            "Epoch 94 : Loss_adam = 0.05477039884501741\n",
            "Epoch 95 : Loss_adam = 0.05477039884501741\n",
            "Epoch 96 : Loss_adam = 0.05477039884501741\n",
            "Epoch 97 : Loss_adam = 0.05477039884501741\n",
            "Epoch 98 : Loss_adam = 0.05477039884501741\n",
            "Epoch 99 : Loss_adam = 0.05477039884501741\n",
            "error =0.05477039884501741\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}